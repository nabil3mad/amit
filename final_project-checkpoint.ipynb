{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9d6d4e-549a-4c81-9022-03a676cb68d7",
   "metadata": {},
   "source": [
    "Imports and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49817f9d-2014-4005-9b7b-eb28916678e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Define paths to the training, validation, and test datasets\n",
    "train_path = \"data/train\"\n",
    "val_path = \"data/val\"\n",
    "test_path = \"data/test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff532fc-6b55-42af-9418-6ad0bdeac81b",
   "metadata": {},
   "source": [
    "Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb60e5f7-ebbd-46da-a803-28bb20fc07f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the batch size\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1e3a9e-1706-411b-a621-c3a860a542f7",
   "metadata": {},
   "source": [
    "Data Augmentation for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4501d46a-67aa-4c6d-aab7-5064edb1a96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing for the training data\n",
    "train_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503fc9c3-6936-451b-b5d1-a67a8547d20e",
   "metadata": {},
   "source": [
    "Load and Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e58d0b0-2ce3-4e78-99f2-ff4f4ff27e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5064 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the training data\n",
    "train_batch = train_datagen.flow_from_directory(\n",
    "    directory=train_path, target_size=(244, 244), batch_size=batch_size, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d9642-5098-4753-9057-af9547fd075b",
   "metadata": {},
   "source": [
    "Preprocessing for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23bfb2df-918e-47dc-b42e-1c6acb96d1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1733 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for the test data without data augmentation\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "test_batch = test_datagen.flow_from_directory(\n",
    "    directory=test_path, \n",
    "    target_size=(244, 244), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c70c3-2266-439f-8af7-01f1dd4d006f",
   "metadata": {},
   "source": [
    "Preprocessing for Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fa959c1-e813-4a49-8094-52ca8f720794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 755 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing for the validation data without data augmentation\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input)\n",
    "val_batch = val_datagen.flow_from_directory(\n",
    "    directory=val_path, \n",
    "    target_size=(244, 244), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f583b9-3214-4657-bf4d-dce6336917d0",
   "metadata": {},
   "source": [
    "Load Pre-trained VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e87d02-bfc9-4a8e-9dc7-b362959df184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model pre-trained on ImageNet, excluding the top classification layer\n",
    "vgg16_model = keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(244, 244, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9532b2e2-a7a0-45f7-8b33-c140eb759105",
   "metadata": {},
   "source": [
    "Freeze VGG16 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7136f662-5c58-44ac-933d-0687874ef0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the layers of the VGG16 model to prevent them from being trained\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd27f10-5366-45d1-8e59-938ac822ce47",
   "metadata": {},
   "source": [
    "Build Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2e631be-b7fb-465b-b1f5-e2e275fe064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the custom model by adding new layers to the VGG16 base model\n",
    "model = keras.Sequential([\n",
    "    vgg16_model,                           # Add the pre-trained VGG16 model\n",
    "    keras.layers.Flatten(),                # Flatten the output from VGG16\n",
    "    keras.layers.Dense(256, activation='relu'),  # Add a fully connected layer with 256 neurons and ReLU activation\n",
    "    keras.layers.Dropout(0.5),             # Add dropout for regularization to prevent overfitting\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Output layer with a single neuron for binary classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e92a1-d501-4b8e-935a-a418ed9fd81a",
   "metadata": {},
   "source": [
    "Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac003c29-b5ff-4054-a487-45645c5550f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with Adam optimizer, binary crossentropy loss, and accuracy metric\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8815b-fada-4c63-bd9a-d981093b2fc5",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6541f15f-fc5f-46a4-a6c5-1db73019f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "80/80 [==============================] - 1436s 18s/step - loss: 1.0635 - accuracy: 0.9619 - val_loss: 0.3768 - val_accuracy: 0.9815\n",
      "Epoch 2/2\n",
      "80/80 [==============================] - 1440s 18s/step - loss: 0.4265 - accuracy: 0.9765 - val_loss: 0.1381 - val_accuracy: 0.9934\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the training data and validate on the validation data\n",
    "result = model.fit(\n",
    "    train_batch, \n",
    "    validation_data=val_batch, \n",
    "    epochs=2, \n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4856f1-f86a-472d-81d6-48509255bdce",
   "metadata": {},
   "source": [
    "Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5cb86ba-f487-43d5-89b1-fa23c812324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 412s 15s/step - loss: 0.5419 - accuracy: 0.9700\n",
      "Test accuracy: 0.9699942469596863\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_batch, verbose=1)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e880045-edd8-4c7a-9c17-783c435c3395",
   "metadata": {},
   "source": [
    "Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1783b926-7b8b-49c0-bb94-42866e49cb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\project\\data\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\project\\data\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"D:\\project\\data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c7a659-9a06-43fc-a545-41eaeea990fb",
   "metadata": {},
   "source": [
    "testing the model on an external photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d370f841-2b01-4ee0-8cb9-d15030c55f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BCF7716A70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001BCF7716A70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 423ms/step\n",
      "Prediction: With Mask\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the trained model\n",
    "model = keras.models.load_model('D:\\project\\data')\n",
    "\n",
    "# Function to preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, (244, 244))\n",
    "    image = image.astype('float32')\n",
    "    image /= 255.0 \n",
    "    # Expand dimensions to create a batch of size 1\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image\n",
    "\n",
    "# Path to the test image\n",
    "test_image_path = 'test.JPEG'\n",
    "\n",
    "# Preprocess the test image\n",
    "test_image = preprocess_image(test_image_path)\n",
    "\n",
    "# Make predictions\n",
    "prediction = model.predict(test_image)\n",
    "\n",
    "# Convert prediction to a readable format\n",
    "predicted_class = \"Without Mask\" if prediction > 0.5 else \"With Mask\"\n",
    "print(\"Prediction:\", predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a16dfce-5ff7-4b99-ae71-1076d1e0390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 432ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 276ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 271ms/step\n",
      "1/1 [==============================] - 0s 293ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 284ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "1/1 [==============================] - 0s 307ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 286ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 292ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 285ms/step\n",
      "1/1 [==============================] - 0s 288ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 302ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 278ms/step\n",
      "1/1 [==============================] - 0s 310ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 291ms/step\n",
      "1/1 [==============================] - 0s 318ms/step\n",
      "1/1 [==============================] - 0s 296ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 294ms/step\n",
      "1/1 [==============================] - 0s 295ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load the trained model\n",
    "model = keras.models.load_model('D:\\\\project\\\\data')\n",
    "\n",
    "# Define the mask detection labels\n",
    "mask_dict = {0: 'Without Mask', 1: 'With Mask'}\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 is the default camera\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Preprocess the frame function\n",
    "def preprocess_frame(frame):\n",
    "    resized_frame = cv2.resize(frame, (244, 244))  # Resize to match model input size\n",
    "    rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    normalized_frame = rgb_frame.astype('float32') / 255.0  # Normalize pixel values\n",
    "    input_data = np.expand_dims(normalized_frame, axis=0)  # Expand dimensions to fit model input\n",
    "    return input_data\n",
    "\n",
    "# Read frames from the webcam\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    input_data = preprocess_frame(frame)\n",
    "\n",
    "    # Perform prediction\n",
    "    prediction = model.predict(input_data)\n",
    "\n",
    "    # Convert prediction to label\n",
    "    mask_label = mask_dict[int(prediction[0] > 0.5)]  # For sigmoid activation, threshold at 0.5\n",
    "\n",
    "    # Display mask label on the frame\n",
    "    cv2.putText(frame, mask_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Webcam Mask Detection', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release video capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a17a8-ff10-4ca0-a1d4-6ef9ab42fc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
